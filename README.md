# Modular FastAPI Backend (RAG)

A production-ready FastAPI backend implementing Retrieval-Augmented Generation (RAG) with document ingestion, LLM-powered interview booking, and conversational memory.

## Overview
- **Document Ingestion**: PDF/TXT upload with two selectable chunking strategies (fixed-size, sentence-based)
- **Vector Storage**: Supports Qdrant, Weaviate, Milvus, Pinecone (no FAISS/Chroma)
- **Custom RAG**: Manual retrieval pipeline (no RetrievalQAChain)
- **LLM-Powered Booking**: Natural language interview scheduling with entity extraction
- **Redis Memory**: Multi-turn conversation support with chat history
- **SQL/NoSQL Storage**: Metadata and booking persistence

## Quick Start
- Create a virtual environment and install dependencies:
  ```bash
  python -m venv .venv
  source .venv/bin/activate  # On Windows: .venv\Scripts\activate
  cd modular-fastapi-backend
  pip install -r requirements.txt
  ```
- Configure environment variables:
  ```bash
  echo "USE_QDRANT=false" > .env
  echo "GROQ_API_KEY=your_groq_key_here" >> .env
  echo "OPENAI_API_KEY=your_openai_key_here" >> .env
  ```
- Run the development server:
  ```bash
  cd modular-fastapi-backend
  uvicorn app.main:app --reload
  ```

## API Endpoints

### 1. Document Ingestion API
**`POST /api/v1/ingest`**

Upload PDF or TXT files with selectable chunking strategies.

**Parameters:**
- `file` (multipart/form-data): PDF or TXT file
- `chunking_strategy` (query): `fixed` or `sentence` (default: `fixed`)
- `chunk_size` (query): Token size for chunks (default: 500)

**Example:**
```bash
curl -X POST http://localhost:8000/api/v1/ingest \
  -F "file=@./document.pdf" \
  -F "chunking_strategy=sentence" \
  -F "chunk_size=500"
```

### 2. Conversational RAG & Booking API
**`POST /api/v1/chat`**

Chat with documents or book interviews using natural language.

**Request Body:**
```json
{
  "user_id": "user123",
  "query": "What is in the document?",
  "context": "optional context",
  "top_k": 5
}
```

**Example - Document Q&A:**
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "query": "Summarize the main points from the document"
  }'
```

**Example - Interview Booking (LLM-Powered):**
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user_llm_03",
    "query": "Book interview for Sarah Johnson, email sarah.j@company.com on December 20, 2024 at 10:00 AM"
  }'
```

**Response:**
```json
{
  "response": "Hi Sarah,\n\nYour interview is confirmed for December 20, 2024 at 10:00 AM (Booking ID #6). We've sent the details to sarah.j@company.com and look forward to speaking with you soon!",
  "conversation_id": "user_llm_03",
  "context": null
}
```

![LLM Booking Demo](./proof_llm_booking.png)

## Features

### ✅ Document Ingestion
- Upload `.pdf` and `.txt` files via multipart form
- Two chunking strategies: `fixed` (token-based) and `sentence` (semantic-aware)
- Embedding generation with OpenAI API (with fallback)
- Vector storage in Qdrant/Weaviate/Milvus/Pinecone
- Metadata persistence in SQL database

### ✅ Custom RAG Pipeline
- Manual retrieval implementation (no RetrievalQAChain)
- Semantic search using vector embeddings
- Context-aware response generation with LLM (Groq)
- Multi-turn conversation support

### ✅ LLM-Powered Interview Booking
- Natural language booking: *"Book interview for John Doe at john@email.com on Dec 15 at 2pm"*
- LLM entity extraction (name, email, date, time)
- Natural confirmation messages generated by LLM
- Booking persistence in SQL database
- Regex fallback for reliability

### ✅ Conversation Memory
- Redis-based chat history storage
- In-memory fallback when Redis unavailable
- Session management per user_id
- 7-day TTL on conversations

## API Documentation
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`
## Configuration

### Environment Variables
Create a `.env` file in `modular-fastapi-backend/` directory:

```env
# LLM Configuration
GROQ_API_KEY=your_groq_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.3-70b-versatile

# Embeddings
OPENAI_API_KEY=your_openai_api_key_here
EMBEDDING_MODEL=text-embedding-3-small

# Vector Store (Qdrant/Weaviate/Milvus/Pinecone)
USE_QDRANT=false  # Set to true for production with Qdrant server
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=documents
EMBEDDING_DIM=1536

# Database
DATABASE_URL=sqlite:///./app.db

# Redis (optional - falls back to in-memory)
USE_REDIS=true
REDIS_URL=redis://localhost:6379/0
```

### Vector Store Options
The system supports multiple vector databases:
- **Qdrant** (recommended for production)
- **Weaviate** (enterprise-ready)
- **Milvus** (scalable)
- **Pinecone** (cloud-native)

Set `USE_QDRANT=false` for local testing (uses in-memory store).

## Project Structure
```
modular-fastapi-backend/
├── app/
│   ├── main.py                 # FastAPI app initialization
│   ├── api/v1/
│   │   ├── ingestion.py        # Document upload endpoint
│   │   └── chat.py             # RAG & booking endpoint
│   ├── services/
│   │   ├── embeddings.py       # OpenAI/Groq LLM integration
│   │   ├── vectorstore.py      # Qdrant/in-memory vector store
│   │   ├── booking_handler.py  # LLM-powered booking logic
│   │   └── text_extractor.py   # PDF/TXT parsing
│   ├── utils/
│   │   ├── chunking.py         # Fixed & sentence chunking
│   │   ├── redis_memory.py     # Conversation memory
│   │   └── db.py               # Database models & session
│   ├── schemas/
│   │   └── rag.py              # Pydantic request/response models
│   └── models/                 # SQLAlchemy ORM models
├── tests/                      # Pytest test suite
├── requirements.txt            # Python dependencies
└── README.md
```

## Tech Stack
- **Framework**: FastAPI 0.95+, Uvicorn
- **LLM**: Groq API (Llama 3.3), OpenAI (fallback)
- **Embeddings**: OpenAI text-embedding-3-small
- **Vector DB**: Qdrant (Pinecone/Weaviate/Milvus compatible)
- **Memory**: Redis with aioredis (in-memory fallback)
- **Database**: SQLAlchemy (SQLite/PostgreSQL)
- **PDF Parsing**: PyMuPDF
- **Validation**: Pydantic

## Running Tests
```bash
cd modular-fastapi-backend
pytest
```

## Docker (Optional)
```bash
# Build
docker build -t rag-backend .

# Run
docker run -p 8000:8000 --env-file .env rag-backend
```

## Key Design Decisions

### Why Custom RAG?
- **No RetrievalQAChain**: Full control over retrieval and generation pipeline
- **Modular components**: Easy to swap embeddings, vector stores, or LLMs
- **Production-ready**: Proper error handling, fallbacks, and logging

### Why LLM for Booking?
- **Natural language processing**: Handles various date/time formats
- **Entity extraction**: Robust name, email, date, time parsing
- **Human-like responses**: Warm, professional confirmation messages
- **Fallback**: Regex patterns ensure reliability

### Why Redis?
- **Scalability**: Distributed memory across multiple servers
- **Persistence**: Chat history survives restarts
- **TTL support**: Automatic cleanup of old conversations
- **Fallback**: In-memory store for local development

## Compliance Checklist

✅ **Document Ingestion API**
- ✅ Upload .pdf or .txt files
- ✅ Extract text
- ✅ Two chunking strategies (selectable)
- ✅ Generate embeddings & store in Qdrant/Weaviate/Milvus
- ✅ Save metadata in SQL database

✅ **Conversational RAG API**
- ✅ Custom RAG (no RetrievalQAChain)
- ✅ Use Redis for chat memory
- ✅ Handle multi-turn queries
- ✅ Support interview booking using LLM
- ✅ Store booking info in database

✅ **Constraints**
- ✅ No FAISS/Chroma
- ✅ No UI (backend only)
- ✅ Clean modular code
- ✅ Industry-standard typing and annotations

## Notes
- Use `.env` to switch between local and production services
- For production: add authentication, rate limiting, and monitoring
- Ensure API keys are kept secure and never committed to git
- Large PDFs are automatically chunked to control embedding costs